{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shraeyas/Fake-Reviews-Detection/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V87qcVd0hEC5"
   },
   "source": [
    "###**Clone Repository from https://github.com/Shraeyas/Fake-Reviews-Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HiqR3Tt_g_6Y"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Shraeyas/Fake-Reviews-Detection\n",
    "%cd Fake-Reviews-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOhNn5ClhM04"
   },
   "source": [
    "###**Install, Create and activate a Python Virtual Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sda60a56hWGc"
   },
   "outputs": [],
   "source": [
    "!pip install virtualenv\n",
    "!virtualenv venv\n",
    "!. venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoJdFJtdiZzA"
   },
   "source": [
    "###**Install Requirements by executing the following command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuCSOpRciir7"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxjB1VbShyWp"
   },
   "source": [
    "###**After Restarting runtime, execute the Following Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysa_0Ua4h47p"
   },
   "outputs": [],
   "source": [
    "%cd Fake-Reviews-Detection\n",
    "!. venv/bin/activate\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8y7TBIzfrK29"
   },
   "source": [
    "---\n",
    "####**Import necessary libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjc9NaMnoSrE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--force-retrain FORCE_RETRAIN]\n",
      "                             [--debug DEBUG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\LENOVO\\AppData\\Roaming\\jupyter\\runtime\\kernel-db5a6392-fa73-4577-9867-983762e3cf14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import codecs\n",
    "from progressBar import printProgressBar\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "#Following code is for debugging\n",
    "\n",
    "try :\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--force-retrain', help=\"Set value to 1 if you wish to retrain the models.\")\n",
    "  parser.add_argument('--debug', help=\"Debug.\")\n",
    "  args = parser.parse_args()\n",
    "  force_retrain = args.force_retrain\n",
    "  debug = args.debug\n",
    "\n",
    "except :\n",
    "  force_retrain = None\n",
    "  debug = None\n",
    "\n",
    "try :\n",
    "    shutil.unpack_archive(\"models.zip\", \"models\", \"zip\")\n",
    "\n",
    "except :\n",
    "    pass\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqcazybmotLK"
   },
   "source": [
    "---\n",
    "#### **Read Dataset**\n",
    "---\n",
    "#### **Dataset Download Link : https://www.kaggle.com/lievgarcia/amazon-reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TmRgGaYgontN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       DOC_ID       LABEL  RATING VERIFIED_PURCHASE PRODUCT_CATEGORY  \\\n",
      "0           1  __label1__       4                 N               PC   \n",
      "1           2  __label1__       4                 Y         Wireless   \n",
      "2           3  __label1__       3                 N             Baby   \n",
      "3           4  __label1__       4                 N  Office Products   \n",
      "4           5  __label1__       4                 N           Beauty   \n",
      "...       ...         ...     ...               ...              ...   \n",
      "20995   20996  __label2__       4                 Y            Shoes   \n",
      "20996   20997  __label2__       4                 Y            Shoes   \n",
      "20997   20998  __label2__       5                 Y            Shoes   \n",
      "20998   20999  __label2__       5                 Y            Shoes   \n",
      "20999   21000  __label2__       4                 Y            Shoes   \n",
      "\n",
      "       PRODUCT_ID                                      PRODUCT_TITLE  \\\n",
      "0      B00008NG7N        Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
      "1      B00LH0Y3NM  Note 3 Battery : Stalion Strength Replacement ...   \n",
      "2      B000I5UZ1Q       Fisher-Price Papasan Cradle Swing, Starlight   \n",
      "3      B003822IRA  Casio MS-80B Standard Function Desktop Calculator   \n",
      "4      B00PWSAXAM  Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
      "...           ...                                                ...   \n",
      "20995  B00BXYM8T8  Madden Girl Women's Gettaw Pump,Red Patent,7.5...   \n",
      "20996  B0014C2ORK  crocs Unisex Classic Clog,Khaki,6 US Men's / 8...   \n",
      "20997  B000EX8CCQ  Minnetonka Men's 703 Leather Laced Softsole Mo...   \n",
      "20998  B00748YHVE    Ariat Womens Unbridled Fatbaby 9 B Powder Brown   \n",
      "20999  B00A46KTLU  VIBRAM FIVEFINGERS SPEED XC MENS HIKING SHOES,...   \n",
      "\n",
      "                                            REVIEW_TITLE  \\\n",
      "0                                                 useful   \n",
      "1                                  New era for batteries   \n",
      "2                               doesn't swing very well.   \n",
      "3                                       Great computing!   \n",
      "4                                  Only use twice a week   \n",
      "...                                                  ...   \n",
      "20995                               wide width is great!   \n",
      "20996                                        Love crocs!   \n",
      "20997  I love moccasins This fit like it was custom m...   \n",
      "20998        This fit well, comfortable, best investment   \n",
      "20999             Love these shoes, except for the laces   \n",
      "\n",
      "                                             REVIEW_TEXT  \n",
      "0      When least you think so, this product will sav...  \n",
      "1      Lithium batteries are something new introduced...  \n",
      "2      I purchased this swing for my baby. She is 6 m...  \n",
      "3      I was looking for an inexpensive desk calcolat...  \n",
      "4      I only use it twice a week and the results are...  \n",
      "...                                                  ...  \n",
      "20995  I bought these for work.  I have high arches, ...  \n",
      "20996  Crocs are one of only two brands of shoes that...  \n",
      "20997  I love moccasins  This fit like it was custom ...  \n",
      "20998  I wish these were a little more durable. I got...  \n",
      "20999  I've been looking for a replacement for my bel...  \n",
      "\n",
      "[21000 rows x 9 columns]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"amazon_dataset_1.csv\", \"r\",encoding='utf-8', errors='ignore') as file_dat:\n",
    "     dataset = pd.read_csv(file_dat)\n",
    "print(dataset)\n",
    "df=dataset\n",
    "len_dataset = math.floor(len(dataset)/1)\n",
    "\n",
    "y = dataset.iloc[:,1:2].values\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x66G7idto7Qf"
   },
   "source": [
    "---\n",
    "#### **Download nltk Libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df = df[['LABEL', 'REVIEW_TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['LABEL'] = df['LABEL'].apply(lambda x: 1 if x=='__label__2' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FI2W6TZ9pJ67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QujuFORmpOLn"
   },
   "source": [
    "---\n",
    "#### **Tokenization and Stemming**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axcx-au6pRJv"
   },
   "outputs": [],
   "source": [
    "print (\"\\nPerforming Tokenization and Stemming.\")\n",
    "load_from_disk = False\n",
    "corpus=[]\n",
    "num = 0\n",
    "\n",
    "for i in range(0, math.floor(len_dataset)) :\n",
    "    if not debug :\n",
    "        printProgressBar(iteration = num, total = len_dataset, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        num = num + 1\n",
    "\n",
    "    if os.path.exists(os.path.join(\"models\", \"corpus.sav\")) and force_retrain == None :\n",
    "        load_from_disk = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['REVIEW_TEXT'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    #print (review)\n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "filename = 'corpus.sav'\n",
    "if load_from_disk == False :\n",
    "    pickle.dump(corpus, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "if load_from_disk :\n",
    "    corpus = pickle.load(open(os.path.join(\"models\", filename), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZDO8UFzpW3C"
   },
   "source": [
    "---\n",
    "#### **Count Vectorization**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V_8IFh8pZan"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=3000)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "y=dataset.iloc[:len_dataset,1]\n",
    "\n",
    "filename = 'countvectorizer.sav'\n",
    "pickle.dump(cv, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMzPsnUjpbyg"
   },
   "source": [
    "---\n",
    "#### **POS Tagging**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqjI06Popkau"
   },
   "outputs": [],
   "source": [
    "def POS_Tagging(sentence):\n",
    "    tagged_list = []\n",
    "    tags = []\n",
    "    count_verbs = 0\n",
    "    count_nouns = 0\n",
    "    text=nltk.word_tokenize(sentence)\n",
    "    tagged_list = (nltk.pos_tag(text))\n",
    "\n",
    "    tags = [x[1] for x in tagged_list]\n",
    "    for each_item in tags:\n",
    "        if each_item in ['VERB','VB','VBN','VBD','VBZ','VBG','VBP']:\n",
    "            count_verbs+=1\n",
    "        elif each_item in ['NOUN','NNP','NN','NUM','NNS','NP','NNPS']:\n",
    "            count_nouns+=1\n",
    "        else:\n",
    "            continue\n",
    "    if count_verbs > count_nouns:\n",
    "        sentence = 'F'\n",
    "    else:\n",
    "        sentence = 'T'\n",
    "\n",
    "    return sentence\n",
    "\n",
    "w, h = 2, len_dataset;\n",
    "pos_tag = [[0 for x in range(w)] for y in range(h)]\n",
    "num = 0\n",
    "\n",
    "load_from_disk = False\n",
    "filename = 'pos_tag.sav'\n",
    "print (\"\\n\\nPerforming POS Tagging.\")\n",
    "for i in range(0,len_dataset):\n",
    "    if not debug :\n",
    "        printProgressBar(iteration = num, total = len_dataset, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        num = num + 1\n",
    "\n",
    "    if os.path.exists(os.path.join(\"models\", filename)) and force_retrain == None :\n",
    "        load_from_disk = True\n",
    "        continue\n",
    "\n",
    "    text = dataset['REVIEW_TEXT'][i]\n",
    "    sentence = POS_Tagging(text)\n",
    "\n",
    "    if sentence == 'T':\n",
    "        pos_tag[i][0] = 1\n",
    "        pos_tag[i][1] = 0\n",
    "        #X[i].insert(1)\n",
    "        #X[i].insert(0)\n",
    "    else:\n",
    "        pos_tag[i][0] = 0\n",
    "        pos_tag[i][1] = 1\n",
    "\n",
    "    #print (pos_tag[i])\n",
    "        #X[i].insert(0)\n",
    "        #X[i].insert(1)\n",
    "\n",
    "\n",
    "if load_from_disk == False :\n",
    "    pickle.dump(pos_tag, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "if load_from_disk :\n",
    "    pos_tag = pickle.load(open(os.path.join(\"models\", filename), 'rb'))\n",
    "\n",
    "X = np.append(X, pos_tag, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWxr_ZHUpqwL"
   },
   "source": [
    "---\n",
    "####**Label Encoding**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42bq7ZOPptIl"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "w, h = 3, len_dataset;\n",
    "new_col = [[0 for x in range(w)] for y in range(h)]\n",
    "num = 0\n",
    "\n",
    "test = dict()\n",
    "test_num = 0\n",
    "\n",
    "for i in range(0, len_dataset):\n",
    "    new_col[i][0] = dataset[\"RATING\"][i]\n",
    "    new_col[i][1] = dataset[\"VERIFIED_PURCHASE\"][i]\n",
    "    new_col[i][2] = dataset[\"PRODUCT_CATEGORY\"][i]\n",
    "\n",
    "    if new_col[i][2] not in test.keys() :\n",
    "        test[new_col[i][2]] = 1\n",
    "        test_num = test_num + 1\n",
    "\n",
    "        #print (new_col[i][2])\n",
    "\n",
    "#print (test_num)\n",
    "\n",
    "new_col = np.array(new_col)\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "new_col[:, 0] = labelEncoder.fit_transform(new_col[:, 0])\n",
    "filename = 'labelencoder_1.sav'\n",
    "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "new_col[:, 1] = labelEncoder.fit_transform(new_col[:, 1])\n",
    "filename = 'labelencoder_2.sav'\n",
    "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "new_col[:, 2] = labelEncoder.fit_transform(new_col[:, 2])\n",
    "filename = 'labelencoder_3.sav'\n",
    "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56DhZmyzpy_N"
   },
   "source": [
    "---\n",
    "####**OneHotEncoder / Column Transformer**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZISni_P0p1e0"
   },
   "outputs": [],
   "source": [
    "ct1 = ColumnTransformer([(\"Rating\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "new_col = ct1.fit_transform(new_col)\n",
    "new_col = new_col.astype(np.float32)\n",
    "filename = 'columntransformer1.sav'\n",
    "pickle.dump(ct1, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "ct2 = ColumnTransformer([(\"Verified Purchase\", OneHotEncoder(), [5])], remainder = 'passthrough')\n",
    "new_col = ct2.fit_transform(new_col)\n",
    "new_col = new_col.astype(np.float32)\n",
    "filename = 'columntransformer2.sav'\n",
    "pickle.dump(ct2, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "ct3 = ColumnTransformer([(\"Category\", OneHotEncoder(), [7])], remainder = 'passthrough')\n",
    "new_col = ct3.fit_transform(new_col)\n",
    "new_col = new_col.toarray()\n",
    "new_col = new_col.astype(np.float32)\n",
    "filename = 'columntransformer3.sav'\n",
    "pickle.dump(ct3, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "new_col = new_col.astype(np.uint8)\n",
    "X = X.astype(np.uint8)\n",
    "X = np.append(X, new_col, axis=1).astype(np.uint8)\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tqsx4rJ_qGgt"
   },
   "source": [
    "---\n",
    "####**Split in Train and Test Set**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oI4JnsNmqJEN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOsYZODvqLXb"
   },
   "source": [
    "---\n",
    "####**Training Classifiers**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hI53PdO-qSQF"
   },
   "outputs": [],
   "source": [
    "print (\"\\n\\nTraining Classifier on Bernoulli Naive Bayes.\")\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoullinb = None\n",
    "if os.path.exists(os.path.join(\"models\", \"bernoullinb.sav\")) and force_retrain == None:\n",
    "    bernoullinb = pickle.load(open(os.path.join(\"models\", \"bernoullinb.sav\"), \"rb\"))\n",
    "\n",
    "else :\n",
    "    bernoullinb = BernoulliNB(alpha = 1.0, binarize = 0.0, fit_prior = True, class_prior = None)\n",
    "    bernoullinb.fit(X_train,y_train)\n",
    "\n",
    "    filename = 'bernoullinb.sav'\n",
    "    pickle.dump(bernoullinb, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "y_pred_bernoulli = bernoullinb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"\\nAccuracy of Bernoulli Naive Bayes is : \")\n",
    "print (accuracy_score(y_test, y_pred_bernoulli) * 100)\n",
    "\n",
    "print (\"\\n\\nTraining Classifier on Support Vector Machine.\")\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "\n",
    "clf = None\n",
    "\n",
    "if os.path.exists(os.path.join(\"models\", \"SVM.sav\")) and force_retrain == None:\n",
    "    clf = pickle.load(open(os.path.join(\"models\", \"SVM.sav\"), \"rb\"))\n",
    "    y_pred_svc = pickle.load(open(os.path.join(\"models\", \"SVM_y_pred.sav\"), \"rb\"))\n",
    "\n",
    "else :\n",
    "    clf = SVC(kernel='rbf')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    filename = 'SVM.sav'\n",
    "    pickle.dump(clf, open(os.path.join(\"models\", filename), 'wb'))\n",
    "\n",
    "    y_pred_svc = clf.predict(X_test)\n",
    "    pickle.dump(y_pred_svc, open(os.path.join(\"models\", \"SVM_y_pred.sav\"), 'wb'))\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "shutil.make_archive(\"models\", 'zip', \"models\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"\\nAccuracy of Support Vector Machine is : \")\n",
    "print(accuracy_score(y_test, y_pred_svc) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncR457cPqXN7"
   },
   "source": [
    "---\n",
    "####**Plot Graphs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyIMWHlPqW8b"
   },
   "outputs": [],
   "source": [
    "from graph import plot2d, plot_comp\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "plot2d(X_test, y_test, y_pred_bernoulli, y_pred_svc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqeDfMyttc4vTeL4mkuvBm",
   "include_colab_link": true,
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
